# Configuration de la Diarization (D√©tection de locuteurs)

## üéØ Qu'est-ce que la diarization ?

La **diarization** permet de d√©tecter automatiquement les changements de locuteurs dans un audio/vid√©o. Cela cr√©era un nouveau sous-titre √† chaque fois qu'une personne diff√©rente parle, rendant les dialogues beaucoup plus lisibles.

**Exemple :**

Sans diarization :
```srt
1
00:00:01,000 --> 00:00:10,000
Bonjour, comment vas-tu? Tr√®s bien merci, et toi?
```

Avec diarization :
```srt
1
00:00:01,000 --> 00:00:03,000
Bonjour, comment vas-tu?

2
00:00:03,500 --> 00:00:10,000
Tr√®s bien merci, et toi?
```

## üìã Pr√©requis

La diarization utilise le mod√®le **Pyannote 3.1** de Hugging Face, qui n√©cessite :

1. **Compte Hugging Face** (gratuit)
2. **Token d'acc√®s** (gratuit)
3. **Acceptation des termes du mod√®le** (une seule fois)

## üîß Installation pas √† pas

### √âtape 1 : Cr√©er un compte Hugging Face

1. Allez sur https://huggingface.co
2. Cliquez sur "Sign Up" (Inscription)
3. Cr√©ez votre compte (gratuit)

### √âtape 2 : G√©n√©rer un token d'acc√®s

1. Connectez-vous √† votre compte Hugging Face
2. Allez sur https://huggingface.co/settings/tokens
3. Cliquez sur "New token" (Nouveau token)
4. Donnez-lui un nom (ex: "whisper-diarization")
5. S√©lectionnez le type "Read" (Lecture)
6. Cliquez sur "Generate token"
7. **COPIEZ LE TOKEN** (vous ne pourrez plus le voir apr√®s !)

### √âtape 3 : Accepter les termes du mod√®le

1. Allez sur https://huggingface.co/pyannote/speaker-diarization-3.1
2. Cliquez sur "Agree and access repository"
3. Acceptez les conditions d'utilisation

### √âtape 4 : Configurer le token dans votre projet

1. Copiez le fichier `.env.example` vers `.env` :
   ```bash
   cd /docker/whisper
   cp .env.example .env
   ```

2. √âditez le fichier `.env` :
   ```bash
   nano .env
   ```

3. Remplacez `your_huggingface_token_here` par votre token :
   ```env
   HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
   ```

4. Sauvegardez et quittez (Ctrl+O, Entr√©e, Ctrl+X)

### √âtape 5 : D√©marrer les services

```bash
cd /docker/whisper
docker compose -f docker-compose-webui.yml up -d --build
```

## ‚úÖ V√©rification

Pour v√©rifier que la diarization fonctionne :

1. V√©rifiez les logs du service :
   ```bash
   docker logs pyannote-diarization
   ```

2. Vous devriez voir :
   ```
   [DIARIZATION] Loading pyannote speaker-diarization-3.1 model...
   [DIARIZATION] Using GPU for diarization
   [DIARIZATION] Model loaded successfully!
   ```

3. Testez le health check :
   ```bash
   curl http://localhost:8001/health
   ```

   R√©ponse attendue :
   ```json
   {
     "status": "ok",
     "model_loaded": true,
     "gpu_available": true
   }
   ```

## üé¨ Utilisation

1. Ouvrez l'interface web : http://localhost:7860
2. Cliquez sur "Sous-titres (SRT)" (mode SRT)
3. **Cochez la case** "D√©tecter les changements de locuteurs (dialogues)"
4. Uploadez votre vid√©o/audio avec dialogue
5. Attendez la fin du traitement

Le fichier SRT g√©n√©r√© aura un nouveau segment √† chaque changement de locuteur !

## üêõ D√©pannage

### Le service ne d√©marre pas

**Erreur** : `HF_TOKEN not set - diarization may not work!`

**Solution** : V√©rifiez que vous avez bien cr√©√© le fichier `.env` avec votre token.

### Mod√®le non charg√©

**Erreur** : `Failed to load diarization model`

**Solutions** :
1. V√©rifiez que vous avez accept√© les termes sur https://huggingface.co/pyannote/speaker-diarization-3.1
2. V√©rifiez que votre token est valide
3. Red√©marrez le conteneur :
   ```bash
   docker restart pyannote-diarization
   ```

### La checkbox n'appara√Æt pas

**Cause** : La checkbox n'appara√Æt qu'en mode SRT

**Solution** : Cliquez d'abord sur "Sous-titres (SRT)" en haut de la page

### Diarization ignor√©e

Si la diarization ne fonctionne pas mais qu'il n'y a pas d'erreur :
1. Le service Pyannote n'est peut-√™tre pas disponible (il continuera sans diarization)
2. V√©rifiez les logs :
   ```bash
   docker logs whisper-webui
   ```

   Vous devriez voir :
   ```
   [DIARIZATION] Detected 2 speakers in 45 segments
   [SPEAKER] Applying speaker segmentation to 23 segments
   ```

## üìä Performance

La diarization ajoute environ **2-3 secondes de traitement par minute d'audio**.

Pour une vid√©o de 10 minutes :
- Transcription seule : ~30 secondes
- Transcription + diarization : ~50 secondes

C'est un bon compromis pour avoir des sous-titres beaucoup plus lisibles !

## ‚ùì FAQ

**Q : Est-ce que √ßa fonctionne avec plusieurs langues ?**
A : Oui, Pyannote fonctionne ind√©pendamment de la langue.

**Q : Combien de locuteurs maximum ?**
A : Pyannote d√©tecte automatiquement le nombre de locuteurs (pas de limite fixe).

**Q : Est-ce que √ßa affiche le nom des personnes ?**
A : Non, volontairement. Le syst√®me d√©tecte juste les changements de voix et cr√©e de nouveaux segments. Pas d'affichage "SPEAKER_00:" dans les sous-titres.

**Q : √áa marche si les gens se coupent la parole ?**
A : Oui ! C'est justement pour √ßa qu'on utilise Pyannote au lieu de d√©tecter juste les pauses.

**Q : C'est gratuit ?**
A : Oui, compl√®tement gratuit (mod√®le open-source + self-hosted).

## üîó Ressources

- Documentation Pyannote : https://github.com/pyannote/pyannote-audio
- Mod√®le utilis√© : https://huggingface.co/pyannote/speaker-diarization-3.1
- Hugging Face tokens : https://huggingface.co/settings/tokens
